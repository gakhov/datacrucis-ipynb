{
 "metadata": {
  "name": "StrataHadoop"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "from pyspark import SparkContext\nsc = SparkContext()",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Import data from the file\n========================="
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "import json\nrdd = sc.textFile(\"stratahadoop-BCN-2014.json\").map(lambda tweet: json.loads(tweet))",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "rdd.count()",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 3,
       "text": "8485"
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "filter tweets that were created during the event\n================================================"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "import pytz\nfrom datetime import datetime, timedelta\n\ncet = pytz.timezone(\"CET\")\nutc = pytz.timezone(\"UTC\")\n\nEVENT_START = datetime(2014, 11, 19, 9, 0, 0, tzinfo=cet).astimezone(utc)\nEVENT_END = datetime(2014, 11, 21, 19, 0, 0, tzinfo=cet).astimezone(utc)\n\ndef created_during_event(tweet):\n    \"\"\"\n    Return True if tweet has been created exactly during the current event.\n\n    \"\"\"\n    created_at = datetime.strptime(tweet[\"created_at\"], \"%a %b %d %H:%M:%S +0000 %Y\")\n    return EVENT_START <= utc.localize(created_at) <= EVENT_END",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "rdd.filter(created_during_event).count()",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 5,
       "text": "5686"
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "filter tweets that were created near the conference's venue\n==========================================================="
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "GEOBOX = (41.4083817701,2.2177593708, 41.4106115707, 2.220836401)\n\ndef has_geo(tweet):\n    \"\"\"\n    Return True if tweet has geo information.\n\n    \"\"\"\n    if not tweet[\"coordinates\"]:\n        return False\n    return True\n\ndef close_to_venue(tweet):\n    \"\"\"\n    Return True if tweet has been created inside the geobox around venue.\n\n    \"\"\"\n    lon, lat = tweet[\"coordinates\"][\"coordinates\"]\n    top_lat, top_lon, bottom_lat, bottom_lon = GEOBOX\n    if lat < top_lat or lat > bottom_lat:\n        return False\n    if lon < top_lon or lon > bottom_lon:\n        return False\n    return True",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "rdd.filter(has_geo).count()",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 7,
       "text": "277"
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "rdd.filter(created_during_event).filter(has_geo).count()",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 8,
       "text": "271"
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "rdd.filter(created_during_event).filter(has_geo).filter(close_to_venue).count()",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 9,
       "text": "173"
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "cache\n======"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "rdd_during_event = rdd.filter(created_during_event).cache()",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "hastags\n========"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "from operator import add\n\nhashtags_during_event = rdd_during_event\\\n    .flatMap(lambda tweet: [h['text'].lower() for h in tweet['entities']['hashtags']])\\\n    .map(lambda hashtag: (hashtag, 1))\\\n    .reduceByKey(add)\\\n    .map(lambda (hashtag, cnt): (cnt, hashtag))\\\n    .sortByKey()\nhashtags_during_event.top(25)",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 11,
       "text": "[(5689, u'stratahadoop'),\n (500, u'bigdata'),\n (187, u'spark'),\n (170, u'hadoop'),\n (112, u'elasticsearch'),\n (103, u'datascience'),\n (82, u'iot'),\n (81, u'strataconf'),\n (76, u'ignite'),\n (71, u'ioth'),\n (70, u'eudatalandscape'),\n (66, u'barcelona'),\n (57, u'sparkcamp'),\n (47, u'data'),\n (36, u'analytics'),\n (30, u'nosql'),\n (25, u'velocityconf'),\n (23, u'stratabarcelona'),\n (23, u'dataviz'),\n (22, u'privacy'),\n (22, u'opendata'),\n (21, u'livestream'),\n (20, u'ibm'),\n (20, u'changetheratio'),\n (20, u'bluemix')]"
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "authors\n========"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "authors_during_event = rdd_during_event\\\n    .map(lambda tweet: (tweet['user']['screen_name'], 1))\\\n    .reduceByKey(add)\\\n    .map(lambda (name, cnt): (cnt, name))\\\n    .sortByKey()\nauthors_during_event.top(25)",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 12,
       "text": "[(609, u'strataconf'),\n (87, u'jbaquerot'),\n (80, u'stevenbeeckman'),\n (77, u'trieloff'),\n (65, u'eudatalandscape'),\n (62, u'duncan3ross'),\n (59, u'kate_ting'),\n (57, u'cgogus'),\n (56, u'andreademarco'),\n (52, u'juantomas'),\n (51, u'kestelyn'),\n (51, u'ClouderaEvents'),\n (50, u'jbenno'),\n (48, u'peeterskris'),\n (44, u'indizen_insight'),\n (43, u'pacoid'),\n (42, u'DaanDebie'),\n (41, u'ch_doig'),\n (40, u'NoSQLDigest'),\n (39, u'DatenAssistance'),\n (38, u'ManfredBo'),\n (36, u'mounialalmas'),\n (35, u'adrianafreitas'),\n (34, u'AureliePols'),\n (33, u'mmaibaum')]"
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def is_not_retweet(tweet):\n    if tweet['text'].startswith('RT '):\n        return False\n    return not tweet.get('retweeted_status')\n\nreal_authors_during_event = rdd_during_event\\\n    .filter(is_not_retweet)\\\n    .map(lambda tweet: (tweet['user']['screen_name'], 1))\\\n    .reduceByKey(add)\\\n    .map(lambda (name, cnt): (cnt, name))\\\n    .sortByKey()\nreal_authors_during_event.top(25)",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 14,
       "text": "[(153, u'strataconf'),\n (57, u'trieloff'),\n (48, u'kestelyn'),\n (48, u'eudatalandscape'),\n (47, u'kate_ting'),\n (47, u'jbenno'),\n (47, u'duncan3ross'),\n (44, u'indizen_insight'),\n (42, u'jbaquerot'),\n (38, u'stevenbeeckman'),\n (38, u'peeterskris'),\n (31, u'DaanDebie'),\n (25, u'ClouderaEvents'),\n (21, u'mmaibaum'),\n (21, u'miguelmalvarez'),\n (21, u'adrianafreitas'),\n (21, u'BMCControlM'),\n (19, u'BartAelterman'),\n (17, u'zahedab'),\n (17, u'padolphs'),\n (17, u'CapdevilaPujol'),\n (16, u'natalinobusa'),\n (16, u'kimknilsson'),\n (16, u'boudicca'),\n (16, u'PeterSpeyer')]"
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "histogram\n========="
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def get_bucket(date):\n    \"\"\"\n    Generate bucket name based on the date.\n    \"\"\"\n    created_at = datetime.strptime(date, \"%a %b %d %H:%M:%S +0000 %Y\")\n    return utc.localize(created_at).astimezone(cet).strftime(\"%Y%m%d%H\")\n\nrdd.filter(created_during_event)\\\n    .map(lambda tweet: (get_bucket(tweet[\"created_at\"]), 1))\\\n    .reduceByKey(add)\\\n    .sortByKey(ascending=True)\\\n    .collect()",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 15,
       "text": "[('2014111909', 89),\n ('2014111910', 73),\n ('2014111911', 82),\n ('2014111912', 106),\n ('2014111913', 39),\n ('2014111914', 78),\n ('2014111915', 79),\n ('2014111916', 80),\n ('2014111917', 92),\n ('2014111918', 136),\n ('2014111919', 28),\n ('2014111920', 21),\n ('2014111921', 29),\n ('2014111922', 16),\n ('2014111923', 21),\n ('2014112000', 8),\n ('2014112001', 14),\n ('2014112002', 5),\n ('2014112003', 1),\n ('2014112004', 4),\n ('2014112006', 8),\n ('2014112007', 11),\n ('2014112008', 52),\n ('2014112009', 274),\n ('2014112010', 594),\n ('2014112011', 309),\n ('2014112012', 259),\n ('2014112013', 136),\n ('2014112014', 283),\n ('2014112015', 140),\n ('2014112016', 195),\n ('2014112017', 215),\n ('2014112018', 114),\n ('2014112019', 67),\n ('2014112020', 45),\n ('2014112021', 39),\n ('2014112022', 49),\n ('2014112023', 29),\n ('2014112100', 34),\n ('2014112101', 12),\n ('2014112102', 13),\n ('2014112103', 8),\n ('2014112104', 4),\n ('2014112105', 19),\n ('2014112106', 11),\n ('2014112107', 9),\n ('2014112108', 54),\n ('2014112109', 204),\n ('2014112110', 432),\n ('2014112111', 252),\n ('2014112112', 91),\n ('2014112113', 96),\n ('2014112114', 142),\n ('2014112115', 112),\n ('2014112116', 147),\n ('2014112117', 150),\n ('2014112118', 76)]"
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "authors profile pictures\n========================"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "rdd.map(lambda tweet: (tweet['user']['screen_name'], tweet['user']['profile_image_url']))\\\n    .map(lambda user: (user, 1))\\\n    .reduceByKey(add)\\\n    .filter(lambda (user, count): count > 10)\\\n    .map(lambda (user, count): user)\\\n    .takeSample(False, 10)",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 16,
       "text": "[(u'voukka',\n  u'http://pbs.twimg.com/profile_images/535453618116517888/gId7m6kz_normal.jpeg'),\n (u'jpalmaer',\n  u'http://pbs.twimg.com/profile_images/521514481/johanpalmaer_normal.jpg'),\n (u'semanticfire',\n  u'http://pbs.twimg.com/profile_images/1490549910/bart_normal.jpg'),\n (u'JMateosGarcia',\n  u'http://pbs.twimg.com/profile_images/501752501599625218/zFsUIvJ3_normal.png'),\n (u'MapR_EMEA',\n  u'http://pbs.twimg.com/profile_images/432031537257664512/-0rh4XDS_normal.png'),\n (u'timoreilly',\n  u'http://pbs.twimg.com/profile_images/2823681988/f4f6f2bed8ab4d5a48dea4b9ea85d5f1_normal.jpeg'),\n (u'DaniScherer',\n  u'http://pbs.twimg.com/profile_images/520402903295467521/8yCjgSU7_normal.jpeg'),\n (u'BartAelterman',\n  u'http://pbs.twimg.com/profile_images/1450705749/Bart2_normal.jpg'),\n (u'Datarella',\n  u'http://pbs.twimg.com/profile_images/378800000539612812/55cc6003e421ebeb5f143a89f2197c8a_normal.jpeg'),\n (u'zillioninfotech',\n  u'http://pbs.twimg.com/profile_images/527137027242733569/wpAhvC_W_normal.png')]"
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "",
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
